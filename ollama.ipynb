{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19411f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.llms import OllamaLLM # type: ignore\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    " # type: ignore\n",
    "\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f943f80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Question: {input_text}\\n\\nAnswer: summerize the text in 50 words.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"Question: {input_text}\n",
    "\n",
    "Answer: summerize the text in 50 words.\"\"\"\n",
    "template \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89f866e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input_text'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input_text'], input_types={}, partial_variables={}, template='Question: {input_text}\\n\\nAnswer: summerize the text in 50 words.'), additional_kwargs={})])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b813b2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OllamaLLM(model=\"gemma3:1b\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ccc72ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model\n",
    "\n",
    "input = \"\"\" \n",
    "\n",
    "OllamaLLM\n",
    "caution\n",
    "You are currently on a page documenting the use of Ollama models as text completion models. Many popular Ollama models are chat completion models.\n",
    "\n",
    "You may be looking for this page instead.\n",
    "\n",
    "This page goes over how to use LangChain to interact with Ollama models.\n",
    "\n",
    "Installation\n",
    "# install package\n",
    "%pip install -U langchain-ollama\n",
    "\n",
    "Setup\n",
    "First, follow these instructions to set up and run a local Ollama instance:\n",
    "\n",
    "Download and install Ollama onto the available supported platforms (including Windows Subsystem for Linux aka WSL, macOS, and Linux)\n",
    "macOS users can install via Homebrew with brew install ollama and start with brew services start ollama\n",
    "Fetch available LLM model via ollama pull <name-of-model>\n",
    "View a list of available models via the model library\n",
    "e.g., ollama pull llama3\n",
    "This will download the default tagged version of the model. Typically, the default points to the latest, smallest sized-parameter model.\n",
    "On Mac, the models will be download to ~/.ollama/models\n",
    "\n",
    "On Linux (or WSL), the models will be stored at /usr/share/ollama/.ollama/models\n",
    "\n",
    "Specify the exact version of the model of interest as such ollama pull vicuna:13b-v1.5-16k-q4_0 (View the various tags for the Vicuna model in this instance)\n",
    "To view all pulled models, use ollama list\n",
    "To chat directly with a model from the command line, use ollama run <name-of-model>\n",
    "View the Ollama documentation for more commands. You can run ollama help in the terminal to see available commands.\n",
    "Usage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "input_user = {\"input_text\": input}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8bddadc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class StoryInfo(BaseModel):\n",
    "    title: str = Field(description=\"give a title to the story\")\n",
    "    overview: str = Field(description=\"gist of the story\")\n",
    "    conclusion: str = Field(description=\"Conclusion to the above given story\")\n",
    "\n",
    "story_about = \"India\"\n",
    "parser = JsonOutputParser(pydantic_object=StoryInfo)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "template = \"Answer the user quer.y.\\n{format_instructions} \\n {query}\\n\",\n",
    "input_variables = [\"query\"],\n",
    "partial_variables ={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "output = chain.invoke({\"query\": story_about})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "00aa8d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "title = output['properties'][\"title\"]\n",
    "overview = output['properties'][\"overview\"]\n",
    "conclusion = output['properties'][\"conclusion\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "29c13ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'description': 'India', 'title': 'India', 'type': 'string'}\n",
      "{'description': 'India is a country in South Asia.', 'title': 'Overview', 'type': 'string'}\n",
      "{'description': 'India is a beautiful country with a rich history and culture.', 'title': 'Conclusion', 'type': 'string'}\n"
     ]
    }
   ],
   "source": [
    "print(title)\n",
    "print(overview)\n",
    "print(conclusion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen_lc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
